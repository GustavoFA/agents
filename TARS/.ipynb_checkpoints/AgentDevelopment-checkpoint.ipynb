{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97adf0c0-d29b-4b55-bc39-f00721fd3523",
   "metadata": {},
   "source": [
    "# Agent 0 - Whisper + Llama3 (Ollama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8028e1e1-c520-4bc1-93f8-a6b0a3c93ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import re\n",
    "import io\n",
    "import json\n",
    "import time\n",
    "import whisper\n",
    "import requests\n",
    "import webrtcvad\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "from IPython.display import Audio\n",
    "import scipy.io.wavfile as wavfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcacf7f-82bc-44b4-96fd-fa7cc22114c9",
   "metadata": {},
   "source": [
    "## Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea20343c-68bc-41ab-acdd-0a15a00bd9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio configuration\n",
    "_DURATION = 5 \n",
    "_SAMPLE_RATE = 16000 # desired sample rate for models such as Whisper\n",
    "_CHANNELS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee8d96c-c241-447d-8b12-a785c7d21d23",
   "metadata": {},
   "source": [
    "### Getting audio from the microphone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "99621f68-b140-425a-a5e2-ecc788ad037f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_audio(duration:int=_DURATION, samprate:int=_SAMPLE_RATE, chns:int=_CHANNELS, _verbose:bool=True) -> np.int16:\n",
    "    if _verbose:\n",
    "        print(f'Recording the microphone audio for {duration} seconds')\n",
    "    audio = sd.rec(int(duration * samprate), samplerate=samprate, channels=chns, dtype='int16')\n",
    "    sd.wait()\n",
    "    if _verbose:\n",
    "        print('Recording finished')\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b6d6f76-968e-4833-938a-af722b28a311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_audio(_audio:np.int16, samprate:int=_SAMPLE_RATE) -> None:\n",
    "    print('Playing the audio')\n",
    "    sd.play(_audio, samprate)\n",
    "    sd.wait()\n",
    "    print('Audio finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085c2589-1b02-4eb4-8a51-b2a74bbdb916",
   "metadata": {},
   "source": [
    "The function below isn't work (detecting voice any time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b27fbf1e-c28a-442f-84fb-e9d3400614e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def automatic_audio_cap(samprate:int=_SAMPLE_RATE, aggress:int=3, silence_timeout:float=1.0, frame_duration_ms:int=30) -> np.float32:\n",
    "    vad = webrtcvad.Vad()\n",
    "    vad.set_mode(aggress)\n",
    "\n",
    "    frame_size = int(samprate * frame_duration_ms / 1000)\n",
    "    _buffer = []\n",
    "    silent_chunks = 0\n",
    "    max_silent_chunks = int(silence_timeout * 1000 / frame_duration_ms)\n",
    "    _recording = False\n",
    "\n",
    "    with sd.InputStream(samplerate=samprate, channels=1, dtype='int16') as stream:\n",
    "        while True:\n",
    "            audio_chunk, _ = stream.read(frame_size)\n",
    "            audio_bytes = audio_chunk.tobytes()\n",
    "\n",
    "            is_speech = vad.is_speech(audio_bytes, samprate)\n",
    "\n",
    "            if not _recording:\n",
    "                if is_speech:\n",
    "                    print('Voice detected. Starting recording')\n",
    "                    _recording = True\n",
    "                    _buffer.append(audio_chunk)\n",
    "            else:\n",
    "                _buffer.append(audio_chunk)\n",
    "                if not is_speech:\n",
    "                    silent_chunks += 1\n",
    "                else:\n",
    "                    silent_chunks = 0\n",
    "                if silent_chunks > max_silent_chunks:\n",
    "                    print('Stopping recording ...')\n",
    "                    break\n",
    "    _audio = np.concatenate(_buffer, axis=0)\n",
    "    _audio = _audio.astype(np.float32) / np.iinfo(np.int16).max\n",
    "    _audio = np.squeeze(_audio)\n",
    "\n",
    "    return _audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce088c4d-ec84-49d4-b154-02702fa1bc6b",
   "metadata": {},
   "source": [
    "#### Testing microphone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0b51a42c-1d22-4bf3-9831-f9797ca31aa9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording the microphone audio for 5 seconds\n",
      "Recording finished\n",
      "Playing the audio\n",
      "Audio finished\n"
     ]
    }
   ],
   "source": [
    "_audio_test = take_audio()\n",
    "play_audio(_audio_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbc8a97-aa36-4871-aa5d-9bd2af7203d0",
   "metadata": {},
   "source": [
    "### Store audio in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "317f2c29-985a-4865-8669-f32ad4d76537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_buffer(_audio:np.int16, samprate:int=_SAMPLE_RATE):\n",
    "    _buffer = io.BytesIO()\n",
    "    wavfile.write(_buffer, samprate, _audio)\n",
    "    _buffer.seek(0)\n",
    "    return _buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f8cae5-6fe8-435d-93b3-fa8fa809995e",
   "metadata": {},
   "source": [
    "#### Testing audio and buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2351494-5562-4615-97f5-fd4ca77c6cb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_buf = audio_buffer(_audio_test)\n",
    "_buf.seek(0)\n",
    "Audio(_buf.read(), rate=_SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1b8197-3fbd-45a9-aa34-d445ec40d6cd",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64fa3426-6d72-45ca-a229-bf049d4b627c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcription_audio(audio, model) -> str:\n",
    "    print('Getting the audio text ...')\n",
    "    result = model.transcribe(audio, fp16=False)\n",
    "    return result['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d0e8f0-dec9-4263-ad85-390c48957b85",
   "metadata": {},
   "source": [
    "#### [Whisper (OpenAI)](https://github.com/openai/whisper/blob/main/README.md)\n",
    "\n",
    "Use to get the audio transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "29a01fc6-4501-4fbb-b25e-3542bb6999f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_model_size = 'small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8e613763-c4f8-480b-9d93-20db25c5e0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 461M/461M [00:42<00:00, 11.4MiB/s]\n"
     ]
    }
   ],
   "source": [
    "_model = whisper.load_model(_model_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29ee5886-ee38-4442-92e9-706ffdb67951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# whisper expects float32\n",
    "def _audio_to_model(_audio):\n",
    "    _model_audio_in = _audio.astype(np.float32) / np.iinfo(_audio.dtype).max\n",
    "    return np.squeeze(_model_audio_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1c5c38db-5118-4119-8314-a43b84a14258",
   "metadata": {},
   "outputs": [],
   "source": [
    "_model_audio = _audio_to_model(_audio_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "73228469-4205-4617-b984-e600ec528548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voice detected. Starting recording\n",
      "Stopping recording ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lol = automatic_audio_cap()\n",
    "res = _model.transcribe(lol, fp16=False)\n",
    "print(res['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5b064fae-ab76-4f09-b3a7-90e42cbfa7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcribe\n",
    "_result = _model.transcribe(_model_audio, fp16=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685f0fdd-6b70-4619-9a6a-7a0eb427d39f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(_result['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2e472564-7991-4661-8158-ee871a3c8014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(model, _audio, language:str='pt') -> str:\n",
    "    print(' Transcribing  audio ...')\n",
    "    _audio = whisper.pad_or_trim(_audio)\n",
    "    _mel_spec = whisper.log_mel_spectrogram(_audio).to(model.device)\n",
    "    _, lang_prob = model.detect_language(_mel_spec)\n",
    "    _options = whisper.DecodingOptions( language='pt',\n",
    "                                        fp16=False,\n",
    "                                      )\n",
    "    _text = whisper.decode(model, _mel_spec, _options)\n",
    "    return _text.text.strip().upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0b6ad98f-c238-4bc9-b7cb-b6ba5f94f9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def searching_for_keyword(model, _keyword:str='JORGE', _timeout:int=3) -> bool:\n",
    "    _audio = take_audio(_timeout, _verbose=True)\n",
    "    _audio = _audio_to_model(_audio)\n",
    "    _text = transcribe_audio(model, _audio)\n",
    "    _text =  re.sub(r'[^A-Za-zÀ-ÖØ-öø-ÿ ]+', '', _text).strip()\n",
    "    for t in _text.split():\n",
    "        if t == _keyword.upper():\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e68d217-a7f9-42a6-928c-cd3e6624604f",
   "metadata": {},
   "source": [
    "#### Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "52c2bfae-3b05-4db9-b18e-1300a4a75890",
   "metadata": {},
   "outputs": [],
   "source": [
    "_OLLAMA_MODEL = 'llama3'\n",
    "_OLLAMA_URL = 'http://localhost:11434/api/generate'\n",
    "_LLM_MEM = []\n",
    "_MAX_MEM_TURNS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "192bcd9e-b8a6-49b6-bf90-ab8d3298fdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mem_lim():\n",
    "    global _LLM_MEM\n",
    "    max_items = _MAX_MEM_TURNS * 2 # User message + assistant message\n",
    "    _LLM_MEM = _LLM_MEM[-max_items:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "022dd7ef-9ebf-44d3-adaa-9eaad07d3a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json2text(_json) -> str:\n",
    "    lines = []\n",
    "    for l in _json:\n",
    "        role = l['role'].capitalize()\n",
    "        content = l['content']\n",
    "        lines.append(f'{role} : {content}')\n",
    "    return '\\n'.join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "68c0cf11-0b08-486c-98f2-87f38c282153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_ollama_with_mem(message:str, model:str=_OLLAMA_MODEL, model_url:str=_OLLAMA_URL, _verbose:bool=True) -> str:\n",
    "\n",
    "    global _LLM_MEM\n",
    "    _LLM_MEM.append({'role' : 'user', 'content' : message})\n",
    "\n",
    "    _full_message = f'{json2text(_LLM_MEM)}\\nAssistant:'\n",
    "\n",
    "    if _verbose: \n",
    "        print(f'Sending message to Ollama model {model} ...')\n",
    "        print(message)\n",
    "        \n",
    "    _payload = {\n",
    "        'model' : model,\n",
    "        'prompt' : _full_message,\n",
    "        'stream' : False\n",
    "    } \n",
    "\n",
    "    _time = time.time()\n",
    "\n",
    "    try:\n",
    "        # Problem with timeout - by increasing the history we increase the model's response time\n",
    "        # implement retry or adaptive timeout \n",
    "        response = requests.post(model_url, json=_payload, timeout=60)\n",
    "        _time_slapsed = time.time() - _time\n",
    "        if _verbose:\n",
    "            print(f'Response received in {_time_slapsed:.2f} seconds')\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            answer = result.get('response', '').strip()\n",
    "\n",
    "            _LLM_MEM.append({'role' : 'assistant', 'content' : answer})\n",
    "\n",
    "            mem_lim()\n",
    "\n",
    "            return answer\n",
    "        else:\n",
    "            raise Exception(f'Error - {response.status_code}')\n",
    "        \n",
    "    except requests.exceptions.Timeout:\n",
    "        raise Exception(\"Request timed out : (\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c6ca3673-2d71-4299-879e-929847592f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_ollama(prompt, model=_OLLAMA_MODEL, temp:float=0.7) -> None:\n",
    "\n",
    "    _headers = {'Content-Type' : 'application/json'}\n",
    "    \n",
    "    print(f'Sending message to Ollama model {model} ...')\n",
    "    _payload = {\n",
    "        'model' : model,\n",
    "        'prompt' : prompt,\n",
    "        'stream' : True,\n",
    "        'temperature' : temp\n",
    "    } \n",
    "\n",
    "    response = requests.post(_OLLAMA_URL, headers=_headers, data=json.dumps(_payload), stream=True)\n",
    "    \n",
    "    print(f'{model.upper()} answer:')\n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            _data = json.loads(line.decode('utf-8'))\n",
    "            if 'done' in _data and _data['done']:\n",
    "                break\n",
    "            print(_data.get('response', ''), end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a8a10a20-d705-48ab-8f88-b3bd87b86ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_streaming_text(_text:str, _delay:float=0.01) -> None:\n",
    "    for char in _text:\n",
    "        print(char, end='', flush=True)\n",
    "        time.sleep(_delay)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ce273fd7-260f-4979-b28a-d6d30218deff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama3_conversation(model) -> None:\n",
    "    # get audio\n",
    "    _audio = take_audio()\n",
    "    # fix data\n",
    "    _audio_fixed = _audio_to_model(_audio)\n",
    "    # transcription\n",
    "    _human_text = transcription_audio(_audio_fixed, model)\n",
    "    # send text\n",
    "    # _answer = question_ollama(_human_text)\n",
    "    _answer = question_ollama_with_mem(_human_text)\n",
    "    # get answer\n",
    "    sim_streaming_text(_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd545a60-5857-4612-a116-2b313b0c934c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_test_conversation_with_command_voice(model):\n",
    "\n",
    "    # reset history\n",
    "    global _LLM\n",
    "    _LLM_MEM = []\n",
    "\n",
    "    while True:\n",
    "       if searching_for_keyword(model):\n",
    "            # get audio\n",
    "            _audio = take_audio()\n",
    "            # fix data\n",
    "            _audio_fixed = _audio_to_model(_audio)\n",
    "            # transcription\n",
    "            _human_text = transcription_audio(_audio_fixed, model)\n",
    "            # send text\n",
    "            # _answer = question_ollama(_human_text)\n",
    "            _answer = question_ollama_with_mem(_human_text)\n",
    "            # get answer\n",
    "            sim_streaming_text(_answer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa0467d-819d-4802-9ec5-265470643a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama3_conversation(_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JUP ENV",
   "language": "python",
   "name": "jupy_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
